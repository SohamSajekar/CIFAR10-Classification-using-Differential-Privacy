{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84af4264",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d40902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Mahos/miniforge3/envs/shm_tf/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from absl import flags\n",
    "import tensorflow as tf\n",
    "import tensorflow_privacy\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential, Model\n",
    "# from livelossplot import PlotLossesKeras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow_addons.layers import GroupNormalization\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Add, LayerNormalization\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow_privacy import DPKerasSGDOptimizer, DPKerasAdamOptimizer, DPKerasAdagradOptimizer\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e685a",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe8bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_transform(train):\n",
    "    train_hog = []\n",
    "    for sample in train:\n",
    "        img = Image.fromarray(np.uint8(sample)).convert('RGB')\n",
    "        img = ImageOps.grayscale(img)\n",
    "        hog_features = hog(img, orientations = 9, pixels_per_cell = (8, 8), \n",
    "                           cells_per_block = (4, 4), block_norm = 'L2', visualize = False)\n",
    "        train_hog.append(hog_features)\n",
    "    train_hog = np.array(train_hog)\n",
    "    return train_hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49da7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(train, newsize):\n",
    "    train_resized = []\n",
    "    for sample in train:\n",
    "        img = Image.fromarray(np.uint8(sample)).convert('RGB').resize(newsize)\n",
    "        train_resized.append(np.array(img))\n",
    "    train_resized = np.array(train_resized)\n",
    "    return train_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e56c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: (50000, 128, 128, 3) (50000, 10)\n",
      "testing: (10000, 128, 128, 3) (10000, 10)\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loading cifar10 image dataset\n",
    "data_train, data_test = cifar10.load_data()\n",
    "x_train, y_train = data_train\n",
    "x_test, y_test = data_test\n",
    "\n",
    "# # Hog transformations of images\n",
    "# x_train_hog = hog_transform(x_train)\n",
    "# x_test_hog = hog_transform(x_test)\n",
    "\n",
    "# Resize images\n",
    "x_train = resize_data(x_train, (128, 128))\n",
    "x_test = resize_data(x_test, (128, 128))\n",
    "\n",
    "# Normalizing pixel values of images\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Flattening images in dataset\n",
    "y_train = y_train.flatten().reshape(y_train.shape[0], 1)\n",
    "y_test = y_test.flatten().reshape(y_test.shape[0], 1)\n",
    "\n",
    "# One hot encoding of labels/target column\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"training:\", x_train.shape, y_train.shape)\n",
    "print(\"testing:\", x_test.shape, y_test.shape)\n",
    "print(\"--------------------------------\")\n",
    "# print(\"hog training:\", x_train_hog.shape, y_train.shape)\n",
    "# print(\"hog testing:\", x_test_hog.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0cd67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da99e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2475b63",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ae1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VGG with transfer learning\n",
    "def pre_trained_vgg():\n",
    "    #Building a VGG model with pretrained weights from cifar100\n",
    "    model = Sequential()\n",
    "    vgg_model = VGG16(include_top=False, weights=\"imagenet\",\n",
    "                      classes=10, pooling=max, input_shape=(32,32,3))\n",
    "    vgg_model.trainable=False\n",
    "    model.add(vgg_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, activation='tanh'))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    model.add(Dense(10))\n",
    "    return model\n",
    "\n",
    "\n",
    "#### HOG + ANN\n",
    "def hog_ann():\n",
    "    ### Feed-forward network\n",
    "    model_input = Input(shape = (144))\n",
    "    d = Dense(32, activation = 'tanh')(model_input)\n",
    "    d = LayerNormalization()(d)\n",
    "    d = Dropout(0.4)(d)\n",
    "    d = Dense(16, activation = 'tanh')(d)\n",
    "    d = LayerNormalization()(d)\n",
    "    d = Dense(16, activation = 'tanh')(d)\n",
    "    d = Dropout(0.4)(d)\n",
    "    d = Dense(8, activation = 'tanh')(d)\n",
    "    d = LayerNormalization()(d)\n",
    "#     d = Dense(8, activation = 'tanh')(d)\n",
    "    model_output = Dense(10)(d)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "#### Custom CNN\n",
    "def cnn():\n",
    "    ## CNN network\n",
    "    model_input = Input(shape = (64, 64, 3))\n",
    "    x1 = Conv2D(32, (8, 8), activation = 'tanh', strides = 2)(model_input)\n",
    "    x1 = LayerNormalization()(x1)\n",
    "    x1 = Dropout(0.4)(x1)\n",
    "    x1 = MaxPooling2D(pool_size = (2,1))(x1)\n",
    "    x2 = Conv2D(32, (4, 4), activation = 'tanh', strides = 2)(x1)\n",
    "    x2 = MaxPooling2D(pool_size = (2,1))(x2)\n",
    "    x2 = LayerNormalization()(x2)\n",
    "    ftn = Flatten()(x2)\n",
    "    ### Feed-forward network\n",
    "    d1 = Dense(16, activation = 'tanh')(ftn)\n",
    "    d1 = Dropout(0.1)(d1)\n",
    "    d2 = Dense(16, activation = 'tanh')(d1)\n",
    "    d2 = LayerNormalization()(d2)\n",
    "    d3 = Dropout(0.1)(d2)\n",
    "    d4 = Dense(8, activation = 'tanh')(d3)\n",
    "    d4 = LayerNormalization()(d4)\n",
    "    d5 = Dense(8, activation = 'tanh')(d4)\n",
    "    model_output = Dense(10, activation = 'softmax')(d5)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "#### Pretrained VGG with custom architecture (ResNet)\n",
    "def custommodel():\n",
    "#     vgg_model = VGG16(include_top=False, weights=\"imagenet\", \n",
    "#                   classes=10, pooling=max, input_shape=(128,128,3))\n",
    "#     # re-structure the model\n",
    "#     vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[5].output)\n",
    "#     vgg_model.trainable=False\n",
    "    model_input = Input(shape = (128, 128, 3))\n",
    "#     vgg_output = vgg_model(model_input, training=False)\n",
    "    x1 = Conv2D(16, (3, 3), activation = 'tanh', strides = 2)(model_input)\n",
    "#     x2 = Conv2D(8, (3, 3), activation = 'tanh', strides = 2)(x1)\n",
    "    x2 = MaxPooling2D(pool_size = (3,3))(x1)\n",
    "    ## Residual Block 1\n",
    "    x3 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x2)\n",
    "    x3 = LayerNormalization()(x3) # normalization layer 1\n",
    "    x4 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x3)\n",
    "#     sk1 = Add()([x2, x4]) # skip connection 1\n",
    "    ## Residual Block 2\n",
    "    x5 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x4)\n",
    "    x5 = LayerNormalization()(x5) # normalization layer 2\n",
    "    x6 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x5)\n",
    "#     sk2 = Add()([sk1, x6]) # skip connection 2\n",
    "    ## Residual Block 3\n",
    "    x7 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x6)\n",
    "    x7 = LayerNormalization()(x7) # normalization layer 3\n",
    "    x8 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x7)\n",
    "#     sk3 = Add()([sk2, x8]) # skip connection 3\n",
    "    ## Residual Block 4\n",
    "    x9 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x8)\n",
    "    x9 = LayerNormalization()(x9) # normalization layer 4\n",
    "    x10 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x9)\n",
    "#     sk4 = Add()([sk3, x10]) # skip connection 4\n",
    "    ## Global Average Pooling layer\n",
    "    avg = GlobalAveragePooling2D()(x10)\n",
    "    ftn = Flatten()(avg)\n",
    "    ## Feed-forward network\n",
    "    d1 = Dense(16, activation = 'tanh')(ftn)\n",
    "    d1 = Dropout(0.4)(d1)\n",
    "    d2 = Dense(16, activation = 'tanh')(d1)\n",
    "    d2 = Dropout(0.4)(d2)\n",
    "    model_output = Dense(10)(d2)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "#### ResNet-10\n",
    "def resnet():\n",
    "    ### Custom ResNet model\n",
    "    model_input = Input(shape = (32, 32, 3))\n",
    "    x1 = Conv2D(64, (3, 3), activation = 'tanh', strides = 2)(model_input)\n",
    "#     x2 = Conv2D(32, (3, 3), activation = 'tanh', strides = 2)(x1)\n",
    "    x2 = MaxPooling2D(pool_size = (3,3))(x1)\n",
    "    ## Residual Block 1\n",
    "    x3 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x2)\n",
    "    x3 = LayerNormalization()(x3) # normalization layer 1\n",
    "    x4 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x3)\n",
    "    sk1 = Add()([x2, x4]) # skip connection 1\n",
    "    ## Residual Block 2\n",
    "    x5 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(sk1)\n",
    "    x5 = LayerNormalization()(x5) # normalization layer 2\n",
    "    x6 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x5)\n",
    "    sk2 = Add()([sk1, x6]) # skip connection 2\n",
    "    ## Residual Block 3\n",
    "    x7 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(sk2)\n",
    "    x7 = LayerNormalization()(x7) # normalization layer 3\n",
    "    x8 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x7)\n",
    "    sk3 = Add()([sk2, x8]) # skip connection 3\n",
    "    ## Residual Block 4\n",
    "    x9 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(sk3)\n",
    "    x9 = LayerNormalization()(x9) # normalization layer 4\n",
    "    x10 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x9)\n",
    "    sk4 = Add()([sk3, x10]) # skip connection 4\n",
    "    ## Global Average Pooling layer\n",
    "    avg = GlobalAveragePooling2D()(sk4)\n",
    "    ftn = Flatten()(avg)\n",
    "    # Feed-forward network\n",
    "    d1 = Dense(16, activation = 'tanh')(ftn)\n",
    "    d2 = Dense(16, activation = 'tanh')(d1)\n",
    "    model_output = Dense(10)(d2)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "#### VGG-8\n",
    "def vgg():\n",
    "    ### Custom VGG model\n",
    "    model_input = Input(shape = (32, 32, 3))\n",
    "    x1 = Conv2D(8, (3, 3), activation = 'tanh', strides = 1)(model_input)\n",
    "#     x2 = Conv2D(64, (3, 3), activation = 'tanh', strides = 1)(x1)\n",
    "    x2 = MaxPooling2D(pool_size = (3,3), strides = 2)(x1)\n",
    "    x3 = Conv2D(16, (3, 3), activation = 'tanh', strides = 1)(x2)\n",
    "    x4 = Conv2D(32, (3, 3), activation = 'tanh', strides = 1)(x3)\n",
    "    x4 = MaxPooling2D(pool_size = (3,3), strides = 2)(x4)\n",
    "#     x5 = Conv2D(32, (3, 3), activation = 'tanh', strides = 1)(x4)\n",
    "#     x6 = Conv2D(32, (3, 3), activation = 'relu', strides = 1)(x5)\n",
    "#     x7 = Conv2D(32, (3, 3), activation = 'relu', strides = 1)(x6)\n",
    "    x7 = MaxPooling2D(pool_size = (3,3), strides = 2)(x4)\n",
    "    ftn = Flatten()(x7)\n",
    "    # Feed-forward network\n",
    "    d1 = Dense(8, activation = 'tanh')(ftn)\n",
    "    d1 = Dropout(0.4)(d1)\n",
    "    d2 = Dense(16, activation = 'tanh')(d1)\n",
    "    d2 = Dropout(0.4)(d2)\n",
    "    model_output = Dense(10)(d2)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45569f99",
   "metadata": {},
   "source": [
    "### Privacy Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c3cbd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2560909204328192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100 \n",
    "batch_size = 250 \n",
    "l2_norm_clip = 1 \n",
    "noise_multiplier = 1.2\n",
    "num_microbatches = batch_size\n",
    "learning_rate = 0.15   \n",
    "delta = 1e-5\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "    raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "    \n",
    "# Compute RDP\n",
    "orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "rdp = compute_rdp(q = batch_size / 50000,\n",
    "                  noise_multiplier = noise_multiplier,\n",
    "                  steps = epochs * 50000 // batch_size,\n",
    "                  orders = orders)\n",
    "# Calculate epsilon\n",
    "epsilon = get_privacy_spent(orders, rdp, target_delta = delta)[0]\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19856d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7d3a885",
   "metadata": {},
   "source": [
    "### Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f6cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer.\n",
    "\n",
    "### Time-based learning rate decay\n",
    "decay = learning_rate / epochs\n",
    "# def lr_time_based_decay(epoch, lr):\n",
    "#     return lr * 1 / (1 + decay * epoch)\n",
    "\n",
    "# define optimizer (dp-sgd) \n",
    "optimizer = DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate,\n",
    "    decay=decay)\n",
    "\n",
    "# # define optimizer (dp-adam)\n",
    "# optimizer = DPKerasAdamOptimizer(\n",
    "#     l2_norm_clip=l2_norm_clip,\n",
    "#     noise_multiplier=noise_multiplier,\n",
    "#     num_microbatches=num_microbatches,\n",
    "#     learning_rate=learning_rate)\n",
    "\n",
    "# # define optimizer (dp-adagrad)\n",
    "# optimizer = DPKerasAdagradOptimizer(\n",
    "#     l2_norm_clip=l2_norm_clip,\n",
    "#     noise_multiplier=noise_multiplier,\n",
    "#     num_microbatches=num_microbatches,\n",
    "#     learning_rate=learning_rate)\n",
    "\n",
    "# Compute loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899a12e",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab3b1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 63, 63, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 21, 21, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 19, 19, 16)        2320      \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 19, 19, 16)       32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 17, 17, 16)        2320      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 15, 16)        2320      \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 15, 15, 16)       32        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 16)        2320      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 11, 11, 16)        2320      \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  (None, 11, 11, 16)       32        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 9, 9, 16)          2320      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 7, 7, 16)          2320      \n",
      "                                                                 \n",
      " layer_normalization_3 (Laye  (None, 7, 7, 16)         32        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 5, 5, 16)          2320      \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 16)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,850\n",
      "Trainable params: 19,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_hog = False\n",
    "## Model\n",
    "# model = pre_trained_vgg()\n",
    "# model_hog = hog_ann()\n",
    "model = custommodel()\n",
    "# model = cnn()\n",
    "# model = resnet()\n",
    "# model = vgg()\n",
    "\n",
    "## HOG_training\n",
    "if model_hog:\n",
    "    x_train = x_train_hog\n",
    "    x_test = x_test_hog \n",
    "    model = model_hog\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c074ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc44a8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 17:39:34.760709: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-06 17:39:34.760852: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-04-06 17:39:36.268273: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-06 17:39:36.293864: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-06 17:39:37.786442: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 17:39:38.138614: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-06 17:39:38.776565: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/50000 [=========>....................] - ETA: 2:23:02 - loss: 2.3802 - acc: 0.1089"
     ]
    }
   ],
   "source": [
    "## Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "## Train model\n",
    "history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test),\n",
    "#                     callbacks=[LearningRateScheduler(lr_time_based_decay, verbose=1)],\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting accuracy/loss graph\n",
    "def get_loss(history_loss):\n",
    "    loss = []\n",
    "    for epoch in history_loss:\n",
    "        loss.append(epoch[-1])\n",
    "    return loss\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = get_loss(history.history['loss'])\n",
    "val_loss = get_loss(history.history['val_loss'])\n",
    "print('train_acc:', round(acc[-1], 3), \"    \", 'train_loss:', round(loss[-1], 3))\n",
    "print('val_acc:', round(val_acc[-1], 3), \"      \", 'val_loss:', round(val_loss[-1], 3))\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "## Accuracy plot\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('model_accuracy')\n",
    "plt.xlabel('epoch')\n",
    "## Loss plot\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Loss')\n",
    "plt.ylabel('model_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e923f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
