{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84af4264",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d40902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from absl import flags\n",
    "import tensorflow as tf\n",
    "import tensorflow_privacy\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow_addons.layers import GroupNormalization\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Add, LayerNormalization\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e685a",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_transform(train):\n",
    "    train_hog = []\n",
    "    for sample in train:\n",
    "        img = Image.fromarray(np.uint8(sample)).convert('RGB')\n",
    "        img = ImageOps.grayscale(img)\n",
    "        hog_features = hog(img, orientations = 9, pixels_per_cell = (2, 2), \n",
    "                           cells_per_block = (1, 1), block_norm = 'L2', visualize = False)\n",
    "        train_hog.append(hog_features)\n",
    "    train_hog = np.array(train_hog)\n",
    "    return train_hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e56c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading cifar10 image dataset\n",
    "data_train, data_test = cifar10.load_data()\n",
    "x_train, y_train = data_train\n",
    "x_test, y_test = data_test\n",
    "\n",
    "# Hog transformations of images\n",
    "x_train_hog = hog_transform(x_train)\n",
    "x_test_hog = hog_transform(x_test)\n",
    "\n",
    "# Normalizing pixel values of images\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Flattening images in dataset\n",
    "y_train = y_train.flatten().reshape(y_train.shape[0], 1)\n",
    "y_test = y_test.flatten().reshape(y_test.shape[0], 1)\n",
    "\n",
    "# One hot encoding of labels/target column\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"training:\", x_train.shape, y_train.shape)\n",
    "print(\"testing:\", x_test.shape, y_test.shape)\n",
    "print(\"--------------------------------\")\n",
    "print(\"hog training:\", x_train_hog.shape, y_train.shape)\n",
    "print(\"hog testing:\", x_test_hog.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2475b63",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VGG with transfer learning\n",
    "def pre_trained_vgg():\n",
    "    #Building a VGG model with pretrained weights from cifar100\n",
    "    model = Sequential()\n",
    "    vgg_model = VGG16(include_top=False, weights=\"cifar100vgg.h5\",\n",
    "                      classes=10, pooling=max, input_shape=(32,32,3))\n",
    "    vgg_model.trainable=False\n",
    "    model.add(vgg_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, activation='tanh'))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "#### HOG + ANN\n",
    "def hog_ann():\n",
    "    ### Feed-forward network\n",
    "    model_input = Input(shape = (2304))\n",
    "    d1 = Dense(8, activation = 'tanh')(model_input)\n",
    "    d1 = Dropout(0.1)(d1)\n",
    "    d2 = Dense(16, activation = 'tanh')(d1)\n",
    "    d2 = LayerNormalization()(d2)\n",
    "    d3 = Dense(16, activation = 'tanh')(d2)\n",
    "    d3 = Dropout(0.1)(d3)\n",
    "    d4 = Dense(8, activation = 'tanh')(d3)\n",
    "    d4 = LayerNormalization()(d4)\n",
    "    d5 = Dense(8, activation = 'tanh')(d4)\n",
    "    model_output = Dense(10, activation = 'softmax')(d5)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "#### Custom CNN\n",
    "def cnn():\n",
    "    ## CNN network\n",
    "    model_input = Input(shape = (32, 32, 3))\n",
    "    x1 = Conv2D(16, (8, 8), activation = 'tanh', strides = 2)(model_input)\n",
    "    x1 = LayerNormalization()(x1)\n",
    "    x1 = Dropout(0.4)(x1)\n",
    "    x1 = MaxPooling2D(pool_size = (2,1))(x1)\n",
    "    x2 = Conv2D(32, (4, 4), activation = 'tanh', strides = 2)(x1)\n",
    "    x2 = MaxPooling2D(pool_size = (2,1))(x2)\n",
    "    x2 = LayerNormalization()(x2)\n",
    "    ftn = Flatten()(x2)\n",
    "    ### Feed-forward network\n",
    "    d1 = Dense(8, activation = 'tanh')(ftn)\n",
    "    d1 = Dropout(0.1)(d1)\n",
    "    d2 = Dense(16, activation = 'tanh')(d1)\n",
    "    d2 = LayerNormalization()(d2)\n",
    "    d3 = Dropout(0.1)(d3)\n",
    "    d4 = Dense(8, activation = 'tanh')(d3)\n",
    "    d4 = LayerNormalization()(d4)\n",
    "    d5 = Dense(8, activation = 'tanh')(d4)\n",
    "    model_output = Dense(10, activation = 'softmax')(d5)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "#### ResNet-10\n",
    "def resnet():\n",
    "    ### Custom ResNet model\n",
    "    model_input = Input(shape = (32, 32, 3))\n",
    "    x1 = Conv2D(64, (3, 3), activation = 'tanh', strides = 2)(model_input)\n",
    "    x2 = Conv2D(64, (3, 3), activation = 'tanh', strides = 2)(x1)\n",
    "    x2 = MaxPooling2D(pool_size = (3,3))(x2)\n",
    "    ## Residual Block 1\n",
    "    x3 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x2)\n",
    "    x3 = LayerNormalization()(x3) # normalization layer 1\n",
    "    x4 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x3)\n",
    "    sk1 = Add()([x2, x4]) # skip connection 1\n",
    "    ## Residual Block 2\n",
    "    x5 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(sk1)\n",
    "    x5 = LayerNormalization()(x5) # normalization layer 2\n",
    "    x6 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x5)\n",
    "    sk2 = Add()([sk1, x6]) # skip connection 2\n",
    "    ## Residual Block 3\n",
    "    x7 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(sk2)\n",
    "    x7 = LayerNormalization()(x7) # normalization layer 3\n",
    "    x8 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x7)\n",
    "    sk3 = Add()([sk2, x8]) # skip connection 3\n",
    "    ## Residual Block 4\n",
    "    x9 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(sk3)\n",
    "    x9 = LayerNormalization()(x9) # normalization layer 4\n",
    "    x10 = Conv2D(64, (3, 3), activation = 'relu', strides = 1)(x9)\n",
    "    sk4 = Add()([sk3, x10]) # skip connection 4\n",
    "    ## Global Average Pooling layer\n",
    "    avg = GlobalAveragePooling2D()(sk4)\n",
    "    ftn = Flatten()(avg)\n",
    "    # Feed-forward network\n",
    "    d1 = Dense(64, activation = 'tanh')(ftn)\n",
    "    d2 = Dense(32, activation = 'tanh')(d1)\n",
    "    model_output = Dense(10, activation = 'softmax')(d2)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "#### VGG-8\n",
    "def vgg():\n",
    "    ### Custom ResNet model\n",
    "    model_input = Input(shape = (32, 32, 3))\n",
    "    x1 = Conv2D(64, (3, 3), activation = 'tanh', strides = 1)(model_input)\n",
    "    x2 = Conv2D(64, (3, 3), activation = 'tanh', strides = 1)(x1)\n",
    "    x2 = MaxPooling2D(pool_size = (3,3), strides = 2)(x2)\n",
    "    x3 = Conv2D(128, (3, 3), activation = 'relu', strides = 1)(x2)\n",
    "    x4 = Conv2D(128, (3, 3), activation = 'relu', strides = 1)(x3)\n",
    "    x4 = MaxPooling2D(pool_size = (3,3), strides = 2)(x4)\n",
    "    x5 = Conv2D(256, (3, 3), activation = 'relu', strides = 1)(x4)\n",
    "    x6 = Conv2D(256, (3, 3), activation = 'relu', strides = 1)(x5)\n",
    "    x7 = Conv2D(256, (3, 3), activation = 'relu', strides = 1)(x6)\n",
    "    x7 = MaxPooling2D(pool_size = (3,3), strides = 2)(x7)\n",
    "    ftn = Flatten()(x7)\n",
    "    # Feed-forward network\n",
    "    d1 = Dense(4096, activation = 'tanh')(ftn)\n",
    "    d1 = Dropout(0.4)(d1)\n",
    "    d2 = Dense(4096, activation = 'tanh')(d1)\n",
    "    d2 = Dropout(0.4)(d2)\n",
    "    model_output = Dense(10, activation = 'softmax')(d2)\n",
    "    model = Model(inputs = model_input, outputs = model_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45569f99",
   "metadata": {},
   "source": [
    "### Privacy Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3cbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 130\n",
    "# batch_size = 500\n",
    "# l2_norm_clip = 1\n",
    "# noise_multiplier = 1.3\n",
    "# num_microbatches = 100\n",
    "# learning_rate = 0.15\n",
    "# delta = 1e-5\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 2000\n",
    "l2_norm_clip = 1.2\n",
    "noise_multiplier = 2.06\n",
    "num_microbatches = 100\n",
    "learning_rate = 0.085\n",
    "delta = 1e-5\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "    raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "    \n",
    "# Compute RDP\n",
    "orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "rdp = compute_rdp(q = batch_size / 50000,\n",
    "                  noise_multiplier = noise_multiplier,\n",
    "                  steps = epochs * 50000 // batch_size,\n",
    "                  orders = orders)\n",
    "# Calculate epsilon\n",
    "epsilon = get_privacy_spent(orders, rdp, target_delta = delta)[0]\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3a885",
   "metadata": {},
   "source": [
    "### Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer (dp-sgd) \n",
    "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "# # define optimizer (dp-adam)\n",
    "# optimizer = tensorflow_privacy.DPKerasAdamOptimizer(\n",
    "#     l2_norm_clip=l2_norm_clip,\n",
    "#     noise_multiplier=noise_multiplier,\n",
    "#     num_microbatches=num_microbatches,\n",
    "#     learning_rate=learning_rate,\n",
    "#     gradient_accumulation_steps=5)\n",
    "\n",
    "# define loss function\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899a12e",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "model = pre_trained_vgg()\n",
    "# model_hog = hog_ann()\n",
    "# model = resnet()\n",
    "# model = vgg()\n",
    "\n",
    "## HOG_training\n",
    "if model_hog:\n",
    "    x_train = x_train_hog\n",
    "    x_test = x_test_hog \n",
    "    model = model_hog\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc44a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "## Train model\n",
    "model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test),\n",
    "          batch_size=batch_size, callbacks=[PlotLossesKeras()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30cd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15beba2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
